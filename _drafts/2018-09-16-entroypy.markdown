---
layout: post
title: Building full text search engine in javascript.
date: '2018-09-16 00:16:00'
tags:
- science
---

http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf

Entropy is the amount of missing information. (information with minus sign)

Claude Shanon defines entropy as a number of possible alternatives. An configuration not only exist on its own but is also correlate with another. Example given is 2 people talking over telephone line. What I'm hearing is corresponding with what you' re hearing. The atom in my brain determines the state of atom in yours. 

In the book about "meaning of life...", entropy is defined as number of microscopic configuration that has a common macroscopic appearence. 

Second law of thermal dynamics states that entropy is always increasing. Which means amount of missing information is increasing. 

That is to say information can't increase by itself, unless external energy is supplied (in form of information)

The world is harder to understand everyday. We understand the world better, as science progress, but the amount of missing information increases? 

Think about a monotheist world where people worship one god, you know what on their mind. Think about a scientific world, people have different explanation? 



Bit is a binary digit



Information theory is related to probability theory. [The world is built on probability]

Or bit is the maximum amount of information that a digit which can assume 2 different values hold, given 2 value have equal probability. 


From "the world is built on probability": entropy is also a measure of irreversibility. Physics process can be categoried into reversible and irreversible process. For irreversible process, entropy is always increasing, according to 2nd law of thermodynamics.